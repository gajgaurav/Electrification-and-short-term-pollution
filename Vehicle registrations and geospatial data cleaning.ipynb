{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a7f2482",
   "metadata": {},
   "source": [
    "In this code file, we use the following files/directories to create the variables used for our regressions:\n",
    "1. All RTO dataframes (2010-23), a directory which contains excel files of monthy RTO level data for more than 1200 RTOs for the time period Jan 2010 - Dec 2023 (link: https://vahan.parivahan.gov.in/vahan4dashboard/vahan/view/reportview.xhtml).\n",
    "\n",
    "2. Updated arcGIS RTO to district matching.xlsx, an excel file which contains the mapping of RTOs to their present districts (link: https://www.arcgis.com/home/item.html?id=79875d04f49241979bed4b7a8e5bfdd8#data). In cases where the website does not contain a particular RTO to district mapping, I map these manually.\n",
    "\n",
    "3. final_mapping.csv, a csv file which maps the arcGIS reported districts to their 2011 counterparts. This is done for consistency purposes and is also done manually. In cases where current districts come from multiple 2011 districts, they will be mapped to the group of districts\n",
    "\n",
    "4. 2011 districts datameet boundaries.zip, a directory which contains a shapefile of the boundaries for districts according to the 2011 census (link: https://projects.datameet.org/maps/districts/). Based on final_mapping.csv, I create new district boundaries (including for group of districts) in latest grouped district boundaries.shp.\n",
    "\n",
    "5. correct district naming.xlsx, an excel file which contains the names of 2011 districts as present in government documents. Again, done for consistency purposes and in case demographic data is to be incorporated later.\n",
    "\n",
    "6. weather data, a directory which contains excel files of multiple weather readings for coordinates across India.\n",
    "\n",
    "7. India urbanness shapefile Columbia.zip, a file which contains the boundaries of areas on their urbanness classification, based on 2011 factors. Urbanness is defined according to GHSL (link: https://sedac.ciesin.columbia.edu/data/set/india-spatial-india-census-2011/data-download).\n",
    "\n",
    "8. higher resolution PM2.5 data monthly files, a directory containing netCDF files with monthly PM2.5 data at coordinates across India (link: https://sites.wustl.edu/acag/datasets/surface-pm2-5/). We use version V5.GL.01.\n",
    "\n",
    "9. Monthly nightlight data (all years), a directory containing satellite images of VIIRS nightlight data across the world. This is at a monthly frequency (link: https://eogdata.mines.edu/nighttime_light/monthly/v10/). Available for April 2012 onwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5384552b",
   "metadata": {},
   "source": [
    "As we progress, one thing to keep in mind is the datetime format in python. Let's say a row corresponds to November, 2023. To save it in a datetime object format, it gets saved as 2023-11-01 by default. This does not mean however that the row was recorded on November 1, 2023. In the final section, when we are merging our datasets, we will be dealing with aggregates (total registrations, average PM2.5, average urban nightlight, etc.) and even though the row would mention the date YYYY-MM-01, it wouldn't mean it's for the first day but instead for the entire month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db0e7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARIES \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import Polygon\n",
    "import zipfile as zf\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from netCDF4 import Dataset\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from netCDF4 import Dataset\n",
    "from rasterio.enums import Resampling\n",
    "import rioxarray as rxr\n",
    "import rasterio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4766c780",
   "metadata": {},
   "source": [
    "# Section 1: Mapping vehicle registration data to 2011 districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2835664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINING A FUNCTION TO EXTRACT RELEVANT VARIABLES FROM THE TABULAR FORMAT OF THE EXCEL FILES\n",
    "def table_to_data(table):\n",
    "    \n",
    "    info = table.columns[0]\n",
    "    \n",
    "       \n",
    "    # TAKING TAIL(-2) EXCLUDES THE TOP 2 NULL VALUES ROWS\n",
    "    table = table.tail(-2)\n",
    "\n",
    "\n",
    "    # CHANGING THE NAME OF THE COLUMN HEAD TO MAKE IT EASIER FOR NAME MANIPULATION LATER ON\n",
    "    table.iloc[0][1] = table.columns[1]\n",
    "    table.iloc[0][-1] = table.columns[-1]\n",
    "\n",
    "    # REPLACING THE COLUMN NAMES\n",
    "    table.columns = table.iloc[0] \n",
    "    table.iloc[0][-1] = table.columns[-1]\n",
    "\n",
    "    table = table.rename(columns = {table.columns[0] : 'serial',\n",
    "                                        table.columns[1] : 'fuel', table.columns[-1] : 'total'})\n",
    "    table = table.drop(columns = ['serial', 'total'])\n",
    "\n",
    "    # GETTING RID OF THE USELESS FIRST ROW\n",
    "    table = table.reset_index(drop = True).loc[1:]\n",
    "\n",
    "    # CONVERTS INTO A LONG FORMAT\n",
    "    table = pd.melt(table, id_vars = ['fuel'],\n",
    "           var_name = 'month', value_name = 'count')\n",
    "    table['info'] = info\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2c2bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE OF WHAT THE EXCEL TABLES LOOK LIKE\n",
    "\n",
    "registration_files = zf.ZipFile(\"All RTO dataframes (2010-23).zip\", 'r')\n",
    "registration_files.extractall('Vehicle registration files')\n",
    "\n",
    "files_list = os.listdir('Vehicle registration files/All RTO dataframes (2010-23)')\n",
    "example_table = pd.read_excel('Vehicle registration files/All RTO dataframes (2010-23)/' + files_list[0])\n",
    "example_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e776ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE OF WHAT THE table_to_data FUNCTION DOES WITH THE EXCEL TABLE AS INPUT\n",
    "table_to_data(example_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552f02ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLYING THE table_to_data FUNCTION TO EACH OF THE 17000 OR SO EXCEL FILES AND ADDING THEM ALL TO A LIST CALLED\n",
    "# tables_list\n",
    "\n",
    "files_list = os.listdir('Vehicle registration files/All RTO dataframes (2010-23)')\n",
    "tables_list = []\n",
    "\n",
    "for file_number in range(0,len(files_list)):\n",
    "    if '__MACOSX' in files_list[file_number]:\n",
    "        pass\n",
    "    elif '.ipynb_checkpoints' in files_list[file_number]:\n",
    "        pass\n",
    "    \n",
    "    else: \n",
    "        current_table = pd.read_excel('Vehicle registration files/All RTO dataframes (2010-23)/' + files_list[file_number])\n",
    "        current_table = table_to_data(current_table)\n",
    "        \n",
    "        tables_list.append(current_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e4ad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERTICALLY APPENDING ALL THE TABLES FROM tables_list INTO ONE MAIN DATAFRAME CALLED main_df\n",
    "\n",
    "main_df = pd.concat(tables_list, axis = 0).reset_index(drop = True)\n",
    "main_df = main_df.drop_duplicates()\n",
    "\n",
    "# CREATING ROWS FOR OTHER RELEVANT VARIABLES USING THE info COLUMN\n",
    "\n",
    "# COLUMNS TO SEPERATE OUT THE INFORMATION \n",
    "\n",
    "main_df['state_year'] = main_df['info'].apply(lambda x: x.split('Fuel Month Wise Data  of')[1].split(',')[-1].strip())\n",
    "main_df['year'] = main_df['info'].apply(lambda x: x.split('(')[-1][:-1].strip())\n",
    "main_df['state'] = main_df['state_year'].apply(lambda x: x.split('(')[0].strip())\n",
    "main_df['rto'] = main_df['info'].apply(lambda x: ''.join(x.split(',')[:-1]).split('Fuel Month Wise Data  of')[1].strip())\n",
    "main_df['rto_code'] = main_df['rto'].apply(lambda x: x.split('-')[-1].strip())\n",
    "main_df['rto_name'] = main_df['rto'].apply(lambda x: ' '.join(x.split('-')[:-1]).strip())\n",
    "main_df['state_symbol'] = main_df['rto_code'].str.extract('([a-zA-Z]+)')\n",
    "main_df = main_df.drop(columns = ['info', 'state_year'])\n",
    "\n",
    "# AP126 DOES NOT HAVE VALUES FOR 2023 AND THEREFORE WE SKIP OUT THIS RTO FROM FURTHER ANALYSIS\n",
    "\n",
    "main_df = main_df[main_df['rto_code'] != 'AP126']\n",
    "\n",
    "# CONVERTING TO INTEGER VALUES FOR CALCULATION\n",
    "\n",
    "main_df['count'] = main_df['count'].apply(lambda x: x.replace(',', ''))\n",
    "main_df['count'] = main_df['count'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de0eb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0afaae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING 2 SEPERATE DATAFRAMES TO COUNT TOTAL MONTHLY REGISTRATIONS AT THE RTO LEVEL AND MONTHLY EV REGISTRATIONS\n",
    "# AT THE RTO LEVEL\n",
    "\n",
    "rto_month_counts = main_df.groupby(by = ['month', 'year', 'rto_name', 'state', 'state_symbol',\n",
    "                                         'rto_code'])['count'].sum().to_frame().reset_index().rename(columns = \n",
    "                                                                                                     {'count' :\n",
    "                                                                                                     'overall_count'})\n",
    "\n",
    "ev_counts = main_df[main_df['fuel'] == 'ELECTRIC(BOV)'][['month', 'year', 'rto_name', 'state', 'state_symbol',\n",
    "                                                'rto_code', 'count']]\n",
    "\n",
    "ev_counts = ev_counts.rename(columns = {'count' : 'ev_count'})\n",
    "\n",
    "# CREATING THE EV SHARES DATAFRAME\n",
    "\n",
    "ev_shares = pd.merge(rto_month_counts, ev_counts, on = ['month', 'year', 'rto_name',\n",
    "                                                        'rto_code', 'state', 'state_symbol'], how = 'outer')\n",
    "\n",
    "\n",
    "# FOR MONTHS WITH NO REGISTRATIONS BUT TO STILL CALCULATE THE FRACTION\n",
    "\n",
    "ev_shares['overall_count'] = ev_shares['overall_count'].replace(0, 1) \n",
    "\n",
    "ev_shares['ev_count'] = ev_shares['ev_count'].fillna(0).astype(int)\n",
    "ev_shares['ev_share'] = ((ev_shares['ev_count']/ev_shares['overall_count'])*100).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974c03f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ev_shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822dffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAPPING EACH OF THE RTOS TO THEIR RESPECTIVE DISTRICTS, WHILE TAKING CARE OF SOME NAMING DISCREPANCIES\n",
    "\n",
    "matching_districts = ev_shares[['rto_name']].drop_duplicates()\n",
    "matching_districts['rto_name'] = matching_districts['rto_name'].apply(lambda x: x.strip())\n",
    "\n",
    "updated_matching = pd.read_excel('Updated arcGIS RTO to district matching.xlsx')\n",
    "updated_matching = updated_matching[['rto_name', 'rto_name_y', 'rto_code', 'district']]\n",
    "full_match = pd.merge(matching_districts, updated_matching, on = 'rto_name')\n",
    "for i in range(len(full_match)):\n",
    "    if full_match['district'][i] == 'Angul':\n",
    "        full_match['district'][i] = 'Anugul'\n",
    "    elif full_match['district'][i] == 'Thoothukudi':\n",
    "        full_match['district'][i] = 'Tuticorin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fef11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e91e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADDING DISTRICT INFORMATION TO THE ev_shares DATAFRAME CREATED A FEW CELLS ABOVE.\n",
    "\n",
    "# COMPARED TO EV_SHARES, WE LOSE 168 OBSERVATIONS AS WE DROP PUNJAB(STA) SINCE WE COULD NOT MATCH IT TO A DISTRICT\n",
    "district_data = pd.merge(ev_shares, full_match, on = ['rto_name'])\n",
    "\n",
    "district_data = district_data[['rto_name', 'rto_code_x', 'district', 'state', 'state_symbol',\n",
    "               'year', 'month', 'overall_count', 'ev_count', 'ev_share']].rename(columns = \n",
    "                                                                                 {'rto_code_x' : 'rto_code'})\n",
    "\n",
    "# CONTAINS THE CURRENT DISTRICT TO 2011 DISTRICT TO 2011 DISTRICT (IN DEMOGRAPHIC FILES) NAME CONVERSION\n",
    "\n",
    "districts_2011 = pd.read_csv('final_mapping.csv').drop(columns = 'Unnamed: 0')\n",
    "\n",
    "# RTOS MAPPED TO DISTRICTS\n",
    "\n",
    "mapped_districts = pd.merge(full_match, districts_2011, left_on = ['district'],\n",
    "         right_on = ['district_current (stage 1)'], how = 'left')\n",
    "\n",
    "mapped_districts = mapped_districts[['rto_name', 'district','district_current (stage 1)',\n",
    "                  'district_2011 (stage 2)']].drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "merged_df1 = pd.merge(district_data, mapped_districts, on = ['rto_name', 'district'])\n",
    "merged_df1['district_2011 (stage 2)'] = merged_df1['district_2011 (stage 2)'] + ', ' + merged_df1['state_symbol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1074e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a303a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FURTHER RECONCILING NAMING DIFFERENCES ACROSS DIFFERENT DATAFRAMES\n",
    "\n",
    "for i in range(len(merged_df1)):\n",
    "    \n",
    "    if merged_df1['district_2011 (stage 2)'][i] == 'Ahmedabad/Bhavnagar, GJ':\n",
    "        merged_df1['district_2011 (stage 2)'][i] = 'Ahmadabad/Bhavnagar, GJ'\n",
    "        \n",
    "    elif merged_df1['district_2011 (stage 2)'][i] == 'Leh (Ladakh), LA':\n",
    "        merged_df1['district_2011 (stage 2)'][i] = 'Leh (Ladakh), JK'\n",
    "        merged_df1['state_symbol'][i] = 'JK'\n",
    "        \n",
    "    elif merged_df1['district_2011 (stage 2)'][i] == 'Kargil, LA':\n",
    "        merged_df1['district_2011 (stage 2)'][i] = 'Kargil, JK'\n",
    "        merged_df1['state_symbol'][i] = 'JK'    \n",
    "    \n",
    "    elif merged_df1['district_2011 (stage 2)'][i] == 'Balrampur, CG':\n",
    "        merged_df1['district_2011 (stage 2)'][i] = 'Surguja, CG'\n",
    "        merged_df1['state_symbol'][i] = 'CG' \n",
    "    \n",
    "    elif merged_df1['district_2011 (stage 2)'][i] == 'East Godavari , AP':\n",
    "        merged_df1['district_2011 (stage 2)'][i] = 'East Godavari, AP'\n",
    "        merged_df1['state_symbol'][i] = 'AP' \n",
    "    \n",
    "    elif merged_df1['district_2011 (stage 2)'][i] == 'Panch Mahals, GJ':\n",
    "        merged_df1['district_2011 (stage 2)'][i] = 'Panchmahal, GJ'\n",
    "        merged_df1['state_symbol'][i] = 'GJ' \n",
    "    \n",
    "    elif merged_df1['district_2011 (stage 2)'][i] == 'Panchmahals, GJ':\n",
    "        merged_df1['district_2011 (stage 2)'][i] = 'Panchmahal, GJ'\n",
    "        merged_df1['state_symbol'][i] = 'GJ'\n",
    "    \n",
    "    elif merged_df1['district_2011 (stage 2)'][i] == 'Panch mahal, GJ':\n",
    "        merged_df1['district_2011 (stage 2)'][i] = 'Panchmahal, GJ'\n",
    "        merged_df1['state_symbol'][i] = 'GJ'\n",
    "    \n",
    "    elif merged_df1['district_2011 (stage 2)'][i] == 'Gurgaon , HR':\n",
    "        merged_df1['district_2011 (stage 2)'][i] = 'Gurgaon, HR'\n",
    "        merged_df1['state_symbol'][i] = 'HR'\n",
    "    \n",
    "    elif merged_df1['district_2011 (stage 2)'][i] == 'Chandigarh, HR':\n",
    "        merged_df1['district_2011 (stage 2)'][i] = 'Chandigarh, CH'\n",
    "        merged_df1['state_symbol'][i] = 'CH'\n",
    "        merged_df1['state'][i] = 'Chandigarh'\n",
    "    \n",
    "    elif merged_df1['district_2011 (stage 2)'][i] == 'Multiple, GJ':\n",
    "        merged_df1['district_2011 (stage 2)'][i] = 'Surendranagar/Rajkot, GJ'\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5992a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTICING THAT SOME RTOS BELONG TO DISTRICTS THAT ARE COMPOSED OF MULTIPLE 2011 DISTRICTS, WE NEED TO MAP THAT\n",
    "# RTO'S FIGURES TO THE AMALGAMATION OF THE MULTIPLE DISTRICTS. IN OUR CONTEXT, THE MAXIMUM DISTRICTS THAT A SINGLE\n",
    "# RTO BELONGS TO IS 2. HENCE, WE CREATE A SECOND COLUMN NAMED district_2011 (stage 2.1) TO KEEP TRACK OF THAT.\n",
    "\n",
    "merged_df1['district_2011 (stage 2.1)'] = np.nan\n",
    "for i in range(len(merged_df1)):\n",
    "    if '/' in merged_df1['district_2011 (stage 2)'][i]:\n",
    "        merged_df1['district_2011 (stage 2.1)'][i] = merged_df1['district_2011 (stage 2)'][i].split('/')[1]\n",
    "    else:\n",
    "        merged_df1['district_2011 (stage 2.1)'][i] = merged_df1['district_2011 (stage 2)'][i]\n",
    "\n",
    "# leaves only 1 district in the first column\n",
    "for i in range(len(merged_df1)):\n",
    "    if '/' in merged_df1['district_2011 (stage 2)'][i]:\n",
    "        merged_df1['district_2011 (stage 2)'][i] = merged_df1['district_2011 (stage 2)'][i].split('/')[0] + ', ' + merged_df1['state_symbol'][i]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    \n",
    "# Function to get unique values from two columns in a row\n",
    "def get_unique_values(row):\n",
    "    return set([row['district_2011 (stage 2)'],row['district_2011 (stage 2.1)']])\n",
    "\n",
    "# Apply the function to create a new column 'unique_districts'\n",
    "merged_df1['unique_districts'] = merged_df1.apply(get_unique_values, axis=1)\n",
    "\n",
    "# create the necessary groupings\n",
    "groupings = []\n",
    "for i in range(len(merged_df1)):\n",
    "    if len(merged_df1['unique_districts'][i]) == 2:\n",
    "        groupings.append(merged_df1['unique_districts'][i])\n",
    "\n",
    "# function to get unique values\n",
    "def unique_elements(list1):\n",
    "\n",
    "# initialize a null list\n",
    "    unique_list = []\n",
    "\n",
    "# traverse for all elements\n",
    "    for x in list1:\n",
    "# check if exists in unique_list or not\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x)\n",
    "    return unique_list\n",
    "\n",
    "unique_elements(groupings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e226f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING THE GROUPINGS BASED ON THE OUTPUT OF THE unique_elements FUNCTION\n",
    "\n",
    "merged_df1['grouped_district'] = np.nan\n",
    "for i in range(len(merged_df1)):\n",
    "    if merged_df1['district_2011 (stage 2)'][i] in {'Ahmadabad, GJ', 'Bhavnagar, GJ'}:\n",
    "        merged_df1['grouped_district'][i] = 'Ahmadabad + Bhavnagar, GJ'\n",
    "    \n",
    "    elif merged_df1['district_2011 (stage 2)'][i] in {'Chittoor, AP', 'Y.S.R., AP'}:\n",
    "        merged_df1['grouped_district'][i] = 'Chittoor + Y.S.R., AP'\n",
    "    \n",
    "    elif merged_df1['district_2011 (stage 2)'][i] in {'Faridabad, HR', 'Gurgaon, HR'}:\n",
    "        merged_df1['grouped_district'][i] = 'Faridabad + Gurgaon, HR'\n",
    "    \n",
    "    elif merged_df1['district_2011 (stage 2)'][i] in {'Kheda, GJ', 'Panchmahal, GJ'}:\n",
    "        merged_df1['grouped_district'][i] = 'Kheda + Panchmahal, GJ'\n",
    "    \n",
    "    elif merged_df1['district_2011 (stage 2)'][i] in {'Budaun, UP', 'Moradabad, UP'}:\n",
    "        merged_df1['grouped_district'][i] = 'Budaun + Moradabad, UP'\n",
    "    \n",
    "    elif merged_df1['district_2011 (stage 2)'][i] in {'Rajkot, GJ', 'Surendranagar, GJ'}:\n",
    "        merged_df1['grouped_district'][i] = 'Rajkot + Surendranagar, GJ'\n",
    "    \n",
    "    elif merged_df1['district_2011 (stage 2)'][i] in {'Rae Bareli, UP', 'Sultanpur, UP'}:\n",
    "        merged_df1['grouped_district'][i] = 'Rae Bareli + Sultanpur, UP'\n",
    "    \n",
    "    elif merged_df1['district_2011 (stage 2)'][i] in {'Srikakulam, AP', 'Vizianagaram, AP'}:\n",
    "        merged_df1['grouped_district'][i] = 'Srikakulam + Vizianagaram, AP'\n",
    "    \n",
    "    else:\n",
    "        merged_df1['grouped_district'][i] = merged_df1['district_2011 (stage 2)'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80edc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING THE EV SHARES FOR EACH GROUPED DISTRICT\n",
    "merged_df2 = merged_df1[['rto_name', 'rto_code', 'grouped_district', 'state', 'state_symbol',\n",
    "           'year', 'month', 'overall_count', 'ev_count', 'ev_share']]\n",
    "\n",
    "merged_df3 = merged_df2.groupby(by = ['grouped_district', 'year', 'state', 'state_symbol',\n",
    "                         'month',]).agg({'overall_count' : 'sum', 'ev_count' : 'sum'}).reset_index()\n",
    "\n",
    "merged_df3['ev_share'] = (merged_df3['ev_count']/merged_df3['overall_count'])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee42e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "registrations_dataset = merged_df3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57a7fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "registrations_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87235a3e",
   "metadata": {},
   "source": [
    "# Section 2: Creating shapefiles for grouped districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba17869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READING THE SHAPEFILE CONTAINING BOUNDARIES FOR INDIVIDUAL DISTRICTS ACCORDING TO 2011 CENSUS\n",
    "\n",
    "shapefiles = zf.ZipFile('2011 districts datameet boundaries.zip', 'r')\n",
    "shapefiles.extractall('district shapefiles')\n",
    "gdf_districts = gpd.read_file('district shapefiles/India-Districts-2011Census.shp')\n",
    "\n",
    "state_symbol_dict = {'Andhra Pradesh' : 'AP', 'Uttar Pradesh' : 'UP', 'Gujarat' : 'GJ', 'Maharashtra' : 'MH',\n",
    "                    'Mizoram' : 'MZ', 'Rajasthan' : 'RJ', 'Kerala' : 'KL', 'Madhya Pradesh' : 'MP',\n",
    "                    'Uttarakhand' : 'UK', 'Haryana' : 'HR', 'Punjab' : 'PB', 'Jammu & Kashmir' : 'JK',\n",
    "                    'Arunanchal Pradesh' : 'AR', 'Odisha' : 'OR', 'Bihar' : 'BR', 'Tamil Nadu' : 'TN',\n",
    "                    'Karnataka' : 'KA', 'Assam' : 'AS', 'West Bengal' : 'WB', 'Chhattisgarh' : 'CG',\n",
    "                     'Himachal Pradesh' : 'HP', 'Manipur' : 'MN', 'Jharkhand' : 'JH', 'NCT of Delhi' : 'DL',\n",
    "                     'Chandigarh' : 'CH', 'Dadara & Nagar Havelli' : 'DD', 'Daman & Diu' : 'DD', 'Tripura' : 'TR',\n",
    "                     'Nagaland' : 'NL', 'Sikkim' : 'SK', 'Meghalaya' : 'ML', 'Puducherry' : 'PY',\n",
    "                     'Lakshadweep' : 'LD', 'Andaman & Nicobar Island' : 'AN', 'Goa' : 'GA'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c96f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b28e8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECONCILING NAMING CONVENTIONS FOR SHAPEFILE MERGING LATER\n",
    "\n",
    "gdf_districts['state_symbol'] = np.nan\n",
    "for i in range(len(gdf_districts)):\n",
    "    gdf_districts['state_symbol'][i] = state_symbol_dict[gdf_districts['ST_NM'][i]]\n",
    "\n",
    "for i in range(len(gdf_districts)):\n",
    "    gdf_districts['DISTRICT'][i] = gdf_districts['DISTRICT'][i] + ', ' + gdf_districts['state_symbol'][i]\n",
    "    \n",
    "df_for_gis_group_mapping = pd.read_excel('correct district naming.xlsx').drop(columns = 'Unnamed: 0').dropna()\n",
    "gdf_districts = gdf_districts.merge(df_for_gis_group_mapping, on = 'DISTRICT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba00047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE GOAL IS TO CREATE BOUNDARIES FOR AMALGAMATED DISTRICTS USING THE BOUNDARIES FOR INDIVIDUAL DISTRICTS. FOR\n",
    "# THAT, WE CREATE 2 SEPERATE SHAPEFILES: merge_set1 CONTAINS BOUNDARIES FOR THE FIRST DISTRICT AND merge_set2 \n",
    "# CONTAINS BOUNDARIES FOR THE SECOND DISTRICT. IN CASES WHERE THE FIRST AND SECOND DISTRICT IS THE SAME, THE\n",
    "# BOUNDARIES ARE ALSO THE SAME\n",
    "\n",
    "merging_partner = merged_df1[['district_2011 (stage 2)', 'district_2011 (stage 2.1)',\n",
    "                              'unique_districts', 'grouped_district']]\n",
    "merging_partner['unique_districts'] = merging_partner['unique_districts'].apply(frozenset)\n",
    "merging_partner = merging_partner.drop_duplicates()\n",
    "\n",
    "merge_set1 = gdf_districts.merge(merging_partner, left_on = 'DISTRICT (FOR MERGING)',\n",
    "                                 right_on = 'district_2011 (stage 2)', how = 'right')\n",
    "\n",
    "merge_set1 = merge_set1[['DISTRICT (FOR MERGING)', 'state_symbol', 'geometry', 'district_2011 (stage 2)',\n",
    "          'district_2011 (stage 2.1)', 'unique_districts', 'grouped_district']]\n",
    "\n",
    "merge_set2 = gdf_districts.merge(merging_partner, left_on = 'DISTRICT (FOR MERGING)',\n",
    "                                 right_on = 'district_2011 (stage 2.1)', how = 'right')\n",
    "\n",
    "merge_set2 = merge_set2[['DISTRICT (FOR MERGING)', 'state_symbol', 'geometry', 'district_2011 (stage 2)',\n",
    "          'district_2011 (stage 2.1)', 'unique_districts', 'grouped_district']]\n",
    "\n",
    "merge_set3 = merge_set1.merge(merge_set2, on = ['grouped_district', 'unique_districts']).reset_index(drop = True)\n",
    "\n",
    "merge_set4 = pd.DataFrame(columns = merge_set3.columns)\n",
    "for i in range(len(merge_set3)):\n",
    "    # EVEN IF THERE IS A ROW WITH AN ENTRY FOR AHMEDABAD, IT NEEDS TO BE MAPPED TO AHMEDABAD + BHAVNAGAR. WE FILTER\n",
    "    # THESE CASES OUT. \n",
    "    if (len(merge_set3['unique_districts'][i]) != 2) & ('+' in merge_set3['grouped_district'][i]):\n",
    "        pass\n",
    "    else:\n",
    "        merge_set4.loc[len(merge_set4)] = merge_set3.iloc[i]\n",
    "        \n",
    "\n",
    "# HERE, WE TAKE A UNION OF THE BOUNDARIES OF THE DIFFERENT DISTRICTS. IN CASE OF ONLY 1 SINGLE UNIQUE DISTRICT,\n",
    "# THE UNION WOULD RETURN THE SAME BOUNDARY.\n",
    "merge_set5 = merge_set4[['grouped_district', 'geometry_x', 'geometry_y', 'state_symbol_x']]\n",
    "merge_set5['group_geometry'] = np.nan\n",
    "for i in range(len(merge_set5)):\n",
    "    poly1 = merge_set5['geometry_x'][i]\n",
    "    poly2 = merge_set5['geometry_y'][i]\n",
    "    poly = poly1.union(poly2)\n",
    "    merge_set5['group_geometry'][i] = poly\n",
    "    \n",
    "merge_set5 = merge_set5.drop(columns = ['geometry_x', 'geometry_y'])\n",
    "merge_set5 = merge_set5.rename(columns = {'state_symbol_x' : 'state_symbol'})\n",
    "\n",
    "merge_set5 = gpd.GeoDataFrame(merge_set5, geometry = 'group_geometry', crs = merge_set1.crs)\n",
    "\n",
    "merge_set5.to_file('grouped district boundaries.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2b5f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_district_boundaries = merge_set5.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590e1624",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_district_boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6c8f76",
   "metadata": {},
   "source": [
    "# Section 3: Meteorological variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a271b2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READING THE DIRECTORY CALLED weather data, APPENDING ALL THE FILES TOGETHER INTO A SINGLE LARGE DATAFRAME CALLED\n",
    "# weather_df.\n",
    "weather_files = os.listdir('weather data')\n",
    "weather_files.sort()\n",
    "\n",
    "weather_df = pd.read_excel('weather data/' + weather_files[0])\n",
    "for i in range(1, len(weather_files)):\n",
    "    to_add_weather_df = pd.read_excel('weather data/' + weather_files[i])\n",
    "    weather_df = pd.concat([weather_df, to_add_weather_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e8befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbc8ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_df CONTAINS COORDINATES AND NOT DISTRICTS. THEREFORE, WE MAP THESE COORDINATES TO THE GROUPED DISTRICTS\n",
    "# WE CREATED IN THE PREVIOUS SECTION.\n",
    "\n",
    "weather_df1 = weather_df.drop_duplicates().reset_index(drop = True)\n",
    "group_gdf = gpd.read_file('latest grouped district boundaries.shp')\n",
    "\n",
    "# maps the coordinates to districts using the intersects method\n",
    "geometry = [Point(lon, lat) for lon, lat in zip(weather_df1['LON'], weather_df1['LAT'])]\n",
    "gdf_weather = gpd.GeoDataFrame(weather_df1, geometry = geometry, crs = group_gdf.crs)\n",
    "gdf_combined = gpd.sjoin(gdf_weather, group_gdf, how = 'right', op = 'intersects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9a0274",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b062698",
   "metadata": {},
   "source": [
    "The resolution of the meteorological variables is very sparse, as the grid is comprised of 0.5 x 0.5 degree squares. The data is present at the corners of these 0.5 x 0.5 degree squares. Due to this sparseness, very small districts end up getting completely skipped out as it is possible that none of the corner points lie within a district's boundaries. In this data, about 90 districts end up getting skipped out. Our strategy to fill the missing data can be summarized in the following steps:\n",
    "1. Establish which districts don't have weather data. In this data, we have 5 weather parameters (temperature, relative humidity, windspeed, dewpoint temperature, precipitation) and any district that doesn't have data for even one parameter doesn't have data for any other parameter either.\n",
    "2. Create empty rows of such districts for each of the 5 parameters so that we can fill them up later on.\n",
    "3. Create a column with the centroid of each of the districts with missing data.\n",
    "4. Of the coordinates which have weather data, see which one is closest to the centroids of such districts (with missing data)\n",
    "5. Fill the missing values with the values from that of the closest points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1017082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEPS 1,2, AND 3: ADDING EMPTY ROWS SO THAT WE CAN FILL THEM LATER ON USING NEAREST POINT IMPUTATION\n",
    "\n",
    "# DROPPING ROWS WHICH AREN'T REQUIRED, # ANN is annual average for a particular parameter\n",
    "gdf_combined = gdf_combined.drop(columns = ['index_left', 'ANN']) \n",
    "df2 = gdf_combined.copy()\n",
    "df2['centroid'] = df2['geometry'].centroid\n",
    "\n",
    "# adding the null rows corresponding to each parameter and year. 5 parameters, hence 5 different times.\n",
    "to_concat = df2[df2['PARAMETER'].isnull()].copy()\n",
    "to_concat['PARAMETER'] = 'T2M'\n",
    "df2 = pd.concat([to_concat, df2],axis = 0)\n",
    "\n",
    "to_concat = df2[df2['PARAMETER'].isnull()].copy()\n",
    "to_concat['PARAMETER'] = 'RH2M'\n",
    "df2 = pd.concat([to_concat, df2],axis = 0)\n",
    "\n",
    "to_concat = df2[df2['PARAMETER'].isnull()].copy()\n",
    "to_concat['PARAMETER'] = 'WS2M'\n",
    "df2 = pd.concat([to_concat, df2],axis = 0)\n",
    "\n",
    "to_concat = df2[df2['PARAMETER'].isnull()].copy()\n",
    "to_concat['PARAMETER'] = 'T2MDEW'\n",
    "df2 = pd.concat([to_concat, df2],axis = 0)\n",
    "\n",
    "to_concat = df2[df2['PARAMETER'].isnull()].copy()\n",
    "to_concat['PARAMETER'] = 'PRECTOTCORR'\n",
    "df2 = pd.concat([to_concat, df2],axis = 0)\n",
    "\n",
    "df2 = df2[df2['PARAMETER'].isnull() == False].copy() # skips out the rows which we used for replication purposes\n",
    "# above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80475e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN THE PREVIOUS CODE BLOCK, WE CREATE NEW ROWS BASED ON THE DISTRICTS \n",
    "\n",
    "months = df2.columns[4:16]  # gets the list of months\n",
    "parameters = df2['PARAMETER'].unique()  # gets the list of parameters\n",
    "\n",
    "new_df1 = df2[df2['YEAR'].isnull() == False].copy() # skips out the useless rows, which we use to create the\n",
    "# required rows and then add to the main dataframe\n",
    "new_df2 = df2[df2['YEAR'].isnull()].copy() \n",
    "\n",
    "years_range = pd.DataFrame({'YEAR': range(2010, 2023)})\n",
    "\n",
    "to_add = pd.merge(years_range, new_df2, how='cross').drop(columns = ['YEAR_y'])\n",
    "\n",
    "to_add = to_add[['PARAMETER', 'YEAR_x'] + list(to_add.columns[2:])].rename(columns = {'YEAR_x' : 'YEAR'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950f807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOME COLUMN RENAMING OPERATIONS AND THEN, ISOLATING THAT PART OF THE DATAFRAME WHICH NEEDS TO BE FILLED USING\n",
    "# NEAREST POINT MATCHING\n",
    "\n",
    "df3 = pd.concat([new_df1, to_add], axis = 0) # done\n",
    "df3['YEAR'] = df3['YEAR'].astype(int)\n",
    "df3 = df3.reset_index(drop = True)\n",
    "\n",
    "# for districts which don't have points, we assign them the centroid's coordinates, and proceed thereafter.\n",
    "df3['LAT'] = df3['LAT'].fillna(df3['centroid'].apply(lambda point: point.y))\n",
    "df3['LON'] = df3['LON'].fillna(df3['centroid'].apply(lambda point: point.x))\n",
    "\n",
    "df3 = df3.rename(columns = {'state_symb' : 'state', 'grouped_di' : 'district'})\n",
    "df3 = pd.melt(df3, id_vars = ['district', 'state', 'geometry', 'LAT', 'LON', 'centroid', 'PARAMETER', 'YEAR'],\n",
    "       var_name = 'month', value_name = 'measure')\n",
    "\n",
    "new_df = df3.drop(columns = ['geometry', 'centroid']).copy()\n",
    "newer_df = new_df.pivot(index = ['district', 'state', 'LAT', 'LON', 'YEAR', 'month'], columns = 'PARAMETER',\n",
    "            values = 'measure').reset_index()\n",
    "newer_df.columns.name = None\n",
    "filling_df = newer_df.copy()\n",
    "filled_df = filling_df.copy()\n",
    "\n",
    "nulls_df = filled_df[filled_df['T2M'].isnull()] # seperate dataframe of nulls which we will populate with\n",
    "# imputed values\n",
    "nulls_df = nulls_df.reset_index(drop = True)\n",
    "filled_df = filled_df[filled_df['T2M'].isna() == False]\n",
    "full_frame = filling_df[['LAT', 'LON']].drop_duplicates().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63edb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEPS 4 AND 5\n",
    "coordinates = full_frame[['LAT', 'LON']].values\n",
    "\n",
    "# Create a Nearest Neighbors model\n",
    "k = 2  # Number of neighbors to consider (adjust as needed)\n",
    "nn_model = NearestNeighbors(n_neighbors=k)\n",
    "nn_model.fit(coordinates)\n",
    "\n",
    "search_frame = nulls_df[['LAT', 'LON']].drop_duplicates().reset_index(drop = True)\n",
    "match_values = filled_df[['LAT', 'LON']].drop_duplicates().reset_index(drop = True).values\n",
    "\n",
    "# creating the frames to match on\n",
    "search_frame['LAT_matched'] = np.nan\n",
    "search_frame['LON_matched'] = np.nan\n",
    "\n",
    "for i in range(len(search_frame)):\n",
    "    to_search = nulls_df[['LAT', 'LON']].drop_duplicates().reset_index(drop = True).values[i]\n",
    "    _, indices = nn_model.kneighbors(to_search.reshape(1,-1))\n",
    "    for j in indices[0]:\n",
    "        if full_frame.loc[j].values in match_values:\n",
    "            matched_coord_index = j\n",
    "            break\n",
    "        else:\n",
    "            pass\n",
    "    search_frame['LAT_matched'][i] = full_frame.loc[matched_coord_index]['LAT']\n",
    "    search_frame['LON_matched'][i] = full_frame.loc[matched_coord_index]['LON']\n",
    "    \n",
    "matched_nulls_df = nulls_df.merge(search_frame, on = ['LAT', 'LON'])\n",
    "matched_nulls_df = matched_nulls_df.rename(columns = {'LAT' : 'LAT_tomatch', 'LON' : 'LON_tomatch',\n",
    "                                                     'LAT_matched' : 'LAT', 'LON_matched' : 'LON'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28c50b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFTER RUNNING THE NEAREST NEIGHBOR ALGORITHM, LAT_matched and LON_matched are the points (for which we have data)\n",
    "# which are the closest to the districts' (with missing data) centroids\n",
    "search_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d366e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_nulls_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7863317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FURTHER FORMATTING AND RENAMING COLUMNS AND VALUES FOR MERGING LATER WITH THE OTHER (VEHICLE REGISTRATIONS,\n",
    "# PM2.5, NIGHTLIGHT DATA) DATAFRAMES\n",
    "matched_nulls_df2 = matched_nulls_df.merge(filled_df, on = ['LAT', 'LON', 'YEAR', 'month'], how = 'inner')\n",
    "\n",
    "for i in range(len(matched_nulls_df2)):\n",
    "    matched_nulls_df2['PRECTOTCORR_x'][i] = matched_nulls_df2['PRECTOTCORR_y'][i]\n",
    "    matched_nulls_df2['RH2M_x'][i] = matched_nulls_df2['RH2M_y'][i]\n",
    "    matched_nulls_df2['T2M_x'][i] = matched_nulls_df2['T2M_y'][i]\n",
    "    matched_nulls_df2['T2MDEW_x'][i] = matched_nulls_df2['T2MDEW_y'][i]\n",
    "    matched_nulls_df2['WS2M_x'][i] = matched_nulls_df2['WS2M_y'][i]\n",
    "\n",
    "matched_nulls_df2 = matched_nulls_df2[['district_x', 'state_x', 'LAT_tomatch', 'LON_tomatch', 'YEAR', 'month', \n",
    "         'PRECTOTCORR_x', 'RH2M_x', 'T2M_x', 'T2MDEW_x', 'WS2M_x']]\n",
    "matched_nulls_df2 = matched_nulls_df2.rename(columns = {'district_x' : 'district', 'state_x' : 'state', 'LAT_tomatch' : 'LAT',\n",
    "                                     'LON_tomatch' : 'LON', 'PRECTOTCORR_x' : 'PRECTOTCORR', 'RH2M_x' : 'RH2M',\n",
    "                                     'T2M_x' : 'T2M', 'T2MDEW_x' : 'T2MDEW', 'WS2M_x' : 'WS2M'})\n",
    "\n",
    "updated_filled_df = pd.concat([filled_df, matched_nulls_df2]).reset_index(drop = True)\n",
    "\n",
    "month_mapping = {'JAN' : 1, 'FEB' : 2, 'MAR' : 3, 'APR' : 4, 'MAY' : 5, 'JUN' : 6, 'JUL' : 7,\n",
    "              'AUG' : 8, 'SEP' : 9, 'OCT' : 10, 'NOV' : 11, 'DEC' : 12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584da202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map month initials to month numbers\n",
    "def map_month_initials(month_initial):\n",
    "    return month_mapping.get(month_initial.upper(), None)\n",
    "\n",
    "# Apply the mapping function to create a new column with month numbers\n",
    "updated_filled_df['month'] = updated_filled_df['month'].apply(map_month_initials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314c718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_filled_df['YEAR'] = updated_filled_df['YEAR'].astype(str)\n",
    "updated_filled_df['month'] = updated_filled_df['month'].astype(str)\n",
    "updated_filled_df['year_month'] = updated_filled_df['YEAR'] + '-' + updated_filled_df['month']\n",
    "updated_filled_df['year_month'] = pd.to_datetime(updated_filled_df['year_month'])\n",
    "updated_filled_df = updated_filled_df.drop(columns = ['YEAR', 'month'])\n",
    "\n",
    "updated_filled_df = updated_filled_df.groupby(by = ['district', 'state', 'year_month']).agg({'PRECTOTCORR' : 'mean',\n",
    "                                                                       'RH2M' : 'mean',\n",
    "                                                                       'T2M' : 'mean',\n",
    "                                                                       'T2MDEW' : 'mean',\n",
    "                                                                       'WS2M' : 'mean'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0db7e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_dataset = updated_filled_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc446475",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8a309e",
   "metadata": {},
   "source": [
    "# Section 4: Urban PM2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ca0043",
   "metadata": {},
   "source": [
    "Electric vehicles are specifically promoted for urban areas, and therefore, it makes sense to measure its impact on urban pollution specifically. NASA classified localities in India into 5 classes of urbanness, based on GHSL. These classes are:\n",
    "1. BUILT UP LAND ONLY\n",
    "2. RURAL\n",
    "3. UNINHABITED\n",
    "4. AGGREMENT\n",
    "5. URBAN PEOPLE ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dc8ed2",
   "metadata": {},
   "source": [
    "To isolate urban areas out of these 5 classes, we focus only on 1,4, and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0da97ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_files = zf.ZipFile('India urbanness shapefile Columbia.zip', 'r')\n",
    "urban_files.extractall('India urbanness shapefiles')\n",
    "\n",
    "file_location = 'India urbanness shapefiles/india-spatial-india-census-2011-census-ghsl-50pct-shp/india-spatial-india-census-2011_census-ghsl-50pct-india.shp'\n",
    "gdf_urban = gpd.read_file(file_location)\n",
    "gdf_urban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e83301",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_drop = [1, 2]\n",
    "\n",
    "# dropping the rural and uninhabited columns as they are redundant once we have the urban areas, and also\n",
    "# we only require their PM2.5 levels for the pollution measure.\n",
    "gdf_urban = gdf_urban.drop(index = rows_to_drop)\n",
    "\n",
    "urban1 = gpd.overlay(grouped_district_boundaries, gdf_urban.tail(2).head(1), how = 'intersection')\n",
    "urban1.to_file('urban1.shp')\n",
    "\n",
    "urban2 = gpd.overlay(grouped_district_boundaries, gdf_urban.tail(1), how = 'intersection')\n",
    "urban2.to_file('urban2.shp')\n",
    "\n",
    "urban3 = gpd.overlay(grouped_district_boundaries, gdf_urban.head(1), how = 'intersection')\n",
    "urban3.to_file('urban3.shp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610d6dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_urbans = gpd.GeoDataFrame(pd.concat([urban1, urban2, urban3], ignore_index = True, axis = 0))\n",
    "\n",
    "all_urbans = all_urbans.groupby(['grouped_district', 'state_symbol'])['geometry'].apply(lambda x: x.unary_union).reset_index()\n",
    "\n",
    "all_urbans.to_file('district urban area shapefiles.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643fc78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_urbans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf710482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE PM2.5 DATA FILES FROM WUSTL CONTAIN DATA FOR ALL COORDINATES WITHIN ASIA. THIS CODE BLOCK ISOLATES THAT TO\n",
    "# JUST COORDINATES BELONGING TO INDIA. GIVEN THAT THE FILES CONTAINS MILLIONS OF POINTS, IT BECOMES IMPORTANT TO\n",
    "# ISOLATE IT JUST TO INDIA FOR COMPUTATIONAL SPEED. ALL THE MONTHLY FILES ARE APPENDED TO THE LIST overall_pm25\n",
    "\n",
    "directory = 'higher resolution PM2.5 data monthly files/'\n",
    "files_list = os.listdir(directory)\n",
    "files_list.sort()\n",
    "\n",
    "pollution_df_file = files_list[27] # starting from april 2012\n",
    "\n",
    "data = Dataset(directory + pollution_df_file)\n",
    "\n",
    "lon_data = data.variables['lon'][:]\n",
    "lat_data = data.variables['lat'][:]\n",
    "pm25_data = data.variables['GWRPM25']\n",
    "\n",
    "lon_data = [ round(lon, 4) for lon in lon_data ]\n",
    "lat_data = [ round(lat, 4) for lat in lat_data ]\n",
    "\n",
    "india_lat = []\n",
    "india_lon = []\n",
    "for lon in lon_data:\n",
    "    if int(lon) in range(68,98):\n",
    "        india_lon.append(lon)\n",
    "\n",
    "for lat in lat_data:\n",
    "    if int(lat) in range(8, 38):\n",
    "        india_lat.append(lat)\n",
    "\n",
    "pollution_df = xr.open_dataset(directory + pollution_df_file)\n",
    "pollution_df = pollution_df.to_dataframe()\n",
    "pollution_df = pollution_df.reset_index()\n",
    "pollution_df['lon'] = pollution_df['lon'].round(4)\n",
    "pollution_df['lat'] = pollution_df['lat'].round(4)\n",
    "pollution_df = pollution_df[(pollution_df['lat'] >= india_lat[0]) & (pollution_df['lat'] <= india_lat[-1])\n",
    "                & (pollution_df['lon'] >= india_lon[0]) & (pollution_df['lon'] <= india_lon[-1])]\n",
    "\n",
    "time_period = pollution_df_file.split('.')[3].split('-')[0]\n",
    "pollution_df = pollution_df.rename(columns = {'GWRPM25' : time_period})\n",
    "\n",
    "overall_pm25 = [pollution_df]\n",
    "\n",
    "for file in files_list[28:]: # continuing from may 2012\n",
    "    month_pollution_df = xr.open_dataset(directory + file)\n",
    "    month_pollution_df = month_pollution_df.to_dataframe()\n",
    "    month_pollution_df = month_pollution_df.reset_index()\n",
    "    month_pollution_df['lon'] = month_pollution_df['lon'].round(4)\n",
    "    month_pollution_df['lat'] = month_pollution_df['lat'].round(4)\n",
    "    month_pollution_df = month_pollution_df[(month_pollution_df['lat'] >= india_lat[0]) & (month_pollution_df['lat'] <= india_lat[-1])\n",
    "                    & (month_pollution_df['lon'] >= india_lon[0]) & (month_pollution_df['lon'] <= india_lon[-1])]\n",
    "    \n",
    "    time_period = file.split('.')[3].split('-')[0]\n",
    "    month_pollution_df = month_pollution_df.rename(columns = {'lon' : 'lon_' + time_period, 'lat' : 'lat_' + time_period,\n",
    "                           'GWRPM25' : time_period})\n",
    "    overall_pm25.append(month_pollution_df[time_period])\n",
    "    \n",
    "    #print(str(files_list.index(file)) + ' ' + time_period + ' done')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bdbc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERTICALLY APPENDING ALL THE FILES PRESENT IN overall_pm25\n",
    "merged_pollution_df = pd.concat(overall_pm25, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fff777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAPPING THE PM2.5 DATA (WHICH IS FOR COORDINATES AND NOT DISTRICTS DIRECTLY) TO THE GROUPED DISTRICT SHAPEFILES\n",
    "spatial_df = merged_pollution_df.iloc[:, :3]\n",
    "spatial_df['lat'] = spatial_df['lat'].astype(float)\n",
    "spatial_df['lon'] = spatial_df['lon'].astype(float)\n",
    "\n",
    "geometry = [Point(lon, lat) for lon, lat in zip(spatial_df['lon'], spatial_df['lat'])]\n",
    "print('geometry created')\n",
    "gdf_ntl = gpd.GeoDataFrame(spatial_df, geometry = geometry, crs = all_urbans.crs)\n",
    "print('dataframe created')\n",
    "gdf_combined = gpd.sjoin(gdf_ntl, all_urbans, how = 'inner', predicate = 'intersects')\n",
    "print('sjoin done')\n",
    "\n",
    "gdf_locations = gdf_combined.copy() # gives you the location reference for each point in the cities\n",
    "urbans_df = merged_pollution_df.loc[gdf_locations.index]\n",
    "gdf_locations = gdf_locations.dropna()[['lon', 'lat', 'district']]\n",
    "urbans_df['lat'] = urbans_df['lat'].astype(float)\n",
    "urbans_df['lon'] = urbans_df['lon'].astype(float)\n",
    "urbans_df = urbans_df.merge(gdf_locations, on = ['lon', 'lat'])\n",
    "\n",
    "urbans_data = pd.melt(urbans_df, id_vars=['lon',\n",
    "                                          'lat', 'district'], var_name = 'year_month', value_name = 'GWRPM25')\n",
    "urbans_data = urbans_data[['district', 'year_month', 'GWRPM25']]\n",
    "urbans_data['year_month'] = urbans_data['year_month'].apply(lambda x: str(x[:4]) + '-' + str(x[4:]))\n",
    "urbans_data['year_month'] = pd.to_datetime(urbans_data['year_month'])\n",
    "urbans_data = urbans_data.groupby(by = ['district', 'year_month'])['GWRPM25'].mean().to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f884931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_pm25_dataset = urbans_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd755a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_pm25_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056fc826",
   "metadata": {},
   "source": [
    "# Section 5: Urban nightlight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c435c35f",
   "metadata": {},
   "source": [
    "When downloaded, the nightlight images are 2GB in size (for each month). to reduce this to a manageable size, we write the below nightlight_data function to accomplish two things mainly:\n",
    "1. To restrict the image's data to India's boundaries. While the 2GB files contain information on all of Asia, this is unnecessary.\n",
    "2. To upscale/downscale the resolution of the pictures. When I upload the images on dropbox, this function helps me reduce the 2GB images to roughly 3 MB each. However, at that level, the resolution is too low to have enough data to map to the districts. Therefore, once the image has been downloaded from dropbox to your system, the below function upscales these images by a factor of 10 (on each dimension) to arrive at 300MB image files. This level is ideal for the level of my units (i.e. districts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108bdfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_element(lst, target):\n",
    "    return min(lst, key = lambda x: abs(x - target))\n",
    "\n",
    "def nightlight_data(tif_file_path, upscale_factor):\n",
    "    \n",
    "    with rasterio.open(tif_file_path) as dataset:\n",
    "        # Resample data to target shape\n",
    "        data = dataset.read(\n",
    "            out_shape=(\n",
    "                dataset.count,\n",
    "                int(dataset.height * upscale_factor),\n",
    "                int(dataset.width * upscale_factor)\n",
    "            ),\n",
    "            resampling=Resampling.average\n",
    "        )\n",
    "\n",
    "        # Scale image transform\n",
    "        transform = dataset.transform * dataset.transform.scale(\n",
    "            (dataset.width / data.shape[-1]),\n",
    "            (dataset.height / data.shape[-2])\n",
    "        )\n",
    "\n",
    "        # Create a new GeoTIFF file for the resampled data\n",
    "        output_file = tif_file_path\n",
    "        with rasterio.open(output_file, 'w', driver='GTiff', height=data.shape[-2], width=data.shape[-1], count=dataset.count, dtype=data.dtype, crs=dataset.crs, transform=transform) as dst:\n",
    "            dst.write(data)\n",
    "\n",
    "    tif_file_path = output_file\n",
    "    dataarray = rxr.open_rasterio(tif_file_path)\n",
    "    dataarray = xr.open_rasterio(tif_file_path)\n",
    "\n",
    "    ntl_df = dataarray[0].to_pandas()\n",
    "\n",
    "    lonlist = list(ntl_df.columns)\n",
    "    latlist = list(ntl_df.index)\n",
    "\n",
    "    lon_lower = str(find_closest_element(lonlist, 68))\n",
    "    lon_upper = str(find_closest_element(lonlist, 98))\n",
    "\n",
    "    lat_lower = str(find_closest_element(latlist, 8))\n",
    "    lat_upper = str(find_closest_element(latlist, 38))\n",
    "\n",
    "    ntl_df.columns = ntl_df.columns.astype(str)\n",
    "    ntl_df.index = ntl_df.index.astype(str)\n",
    "\n",
    "    lon_lower_index = ntl_df.columns.get_loc(lon_lower)\n",
    "    lon_upper_index = ntl_df.columns.get_loc(lon_upper)\n",
    "\n",
    "    lat_lower_index = ntl_df.index.get_loc(lat_lower)\n",
    "    lat_upper_index = ntl_df.index.get_loc(lat_upper)\n",
    "\n",
    "    ntl_df = ntl_df.iloc[:, lon_lower_index : lon_upper_index + 1]\n",
    "    ntl_df = ntl_df.iloc[lat_upper_index : lat_lower_index + 1, :]\n",
    "\n",
    "    exp_ntl_df = ntl_df.reset_index()\n",
    "    exp_ntl_df = exp_ntl_df.rename(columns = {'y' : 'lat'})\n",
    "    to_melt_list = list(exp_ntl_df.columns[1:])\n",
    "    exp_ntl_df = pd.melt(exp_ntl_df, id_vars = ['lat'], value_vars = to_melt_list).rename(columns = {'x' : 'lon',\n",
    "                                                                                            'value' : 'nightlight'})\n",
    "    \n",
    "    year_month = tif_file_path.split('-')[0].split('_')[-1][:6]\n",
    "    exp_ntl_df = exp_ntl_df.rename(columns = {'lat' : 'lat_' + year_month,\n",
    "                                      'lon' : 'lon_' + year_month,\n",
    "                                     'nightlight' : year_month})\n",
    "    #exp_ntl_df.to_csv('nightlight_data - ' + exp_ntl_df['year_month'][0] + '.csv')\n",
    "    return (exp_ntl_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7c9ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE CODE READS AND APPENDS EACH MONTH'S NIGHTLIGHT DATA TO NTL_DF_LIST. COMBINED_DF APPENDS ALL THE NIGHTLIGHT\n",
    "# DATA FROM NTL_DF_LIST\n",
    "\n",
    "files_list = os.listdir('Monthly nightlight data (all years)')\n",
    "files_list.sort()\n",
    "\n",
    "df_file = files_list[0] # starting from april 2012\n",
    "directory = 'Monthly nightlight data (all years)/'\n",
    "\n",
    "main_df_file = directory + df_file\n",
    "main_df = nightlight_data(main_df_file, 10)\n",
    "\n",
    "print(main_df.columns[2] + ' done')\n",
    "\n",
    "ntl_df_list = [main_df]\n",
    "for i in files_list[1:]:\n",
    "    month_df_file = directory + i\n",
    "    data_column = i.split('-')[0].split('_')[-1][:6]\n",
    "    month_df = nightlight_data(month_df_file, 10) \n",
    "    ntl_df_list.append(month_df[data_column])\n",
    "    print(data_column + ' done')\n",
    "\n",
    "combined_df = pd.concat(ntl_df_list, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badeb17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAPS THE COORDINATES TO THE DISTRICTS, AS IN THE PREVIOUS CASE.\n",
    "spatial_df = combined_df.iloc[:, :3]\n",
    "spatial_df = spatial_df.rename(columns = {'lat_202208' : 'lat', 'lon_202208' : 'lon'})\n",
    "spatial_df['lat'] = spatial_df['lat'].astype(float)\n",
    "spatial_df['lon'] = spatial_df['lon'].astype(float)\n",
    "\n",
    "geometry = [Point(lon, lat) for lon, lat in zip(spatial_df['lon'], spatial_df['lat'])]\n",
    "print('geometry created')\n",
    "gdf_ntl = gpd.GeoDataFrame(spatial_df, geometry = geometry, crs = all_urbans.crs)\n",
    "print('dataframe created')\n",
    "gdf_combined = gpd.sjoin(gdf_ntl, all_urbans, how = 'inner', predicate = 'intersects')\n",
    "print('sjoin done')\n",
    "\n",
    "gdf_locations = gdf_combined.copy() # gives you the location reference for each point in the cities\n",
    "urban_ntl_df = combined_df.loc[gdf_locations.index]\n",
    "gdf_locations = gdf_locations.dropna()[['lon', 'lat', 'district']]\n",
    "urban_ntl_df = urban_ntl_df.rename(columns = {'lat_202208' : 'lat', 'lon_202208' : 'lon'})\n",
    "urban_ntl_df['lat'] = urban_ntl_df['lat'].astype(float)\n",
    "urban_ntl_df['lon'] = urban_ntl_df['lon'].astype(float)\n",
    "urban_ntl_df = urban_ntl_df.merge(gdf_locations, on = ['lon', 'lat'])\n",
    "\n",
    "urban_ntl_data = pd.melt(urban_ntl_df, id_vars=['lon',\n",
    "                                          'lat', 'district'], var_name = 'year_month', value_name = 'nightlight')\n",
    "urban_ntl_data = urban_ntl_data[['district', 'year_month', 'nightlight']]\n",
    "urban_ntl_data['year_month'] = urban_ntl_data['year_month'].apply(lambda x: str(x[:4]) + '-' + str(x[4:]))\n",
    "urban_ntl_data['year_month'] = pd.to_datetime(urban_ntl_data['year_month'])\n",
    "urban_ntl_data = urban_ntl_data.groupby(by = ['district', 'year_month'])['nightlight'].mean().to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e669f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_ntl_dataset = urban_ntl_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe61dae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_ntl_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3861751",
   "metadata": {},
   "source": [
    "# Merging the created datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a88387",
   "metadata": {},
   "source": [
    "Based on grouped_district_boundaries, we merge the following 4 datasets:\n",
    "1. registrations_dataset\n",
    "2. weather_dataset\n",
    "3. urban_pm25_dataset\n",
    "4. urban_ntl_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b2af7f",
   "metadata": {},
   "source": [
    "We merge the attributes from each of the datasets on the district and year-month column. Since the naming of attributes is different only in registrations_dataset and same in the others, we make the following modifications to registrations_dataset:\n",
    "1. rename grouped_district to district\n",
    "2. bring year and month together into a column called year_month, with the same format as the other datasets (using month_mapping, created towards the end of section 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1a6181",
   "metadata": {},
   "outputs": [],
   "source": [
    "registrations_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c84133",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0e6a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_pm25_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dbd362",
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_ntl_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff97288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "registrations_dataset = registrations_dataset.rename(columns = {'grouped_district' : 'district'})\n",
    "registrations_dataset['month'] = registrations_dataset['month'].apply(lambda x: month_mapping[x])\n",
    "registrations_dataset['year_month'] = registrations_dataset['year'].astype(str) + '-' + registrations_dataset['month'].astype(str)\n",
    "registrations_dataset['year_month'] = pd.to_datetime(registrations_dataset['year_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199bfe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dataset = pd.merge(registrations_dataset, weather_dataset,\n",
    "                          on = ['district', 'year_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a9a4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dataset = pd.merge(master_dataset, urban_pm25_dataset,\n",
    "                          on = ['district', 'year_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764e6463",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dataset = pd.merge(master_dataset, urban_ntl_dataset,\n",
    "                          on = ['district', 'year_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adde503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dataset = master_dataset.drop(columns = ['state_x', 'state_y', 'year', 'month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c8d88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dataset.to_csv('Dataset for EV and pollution paper.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
